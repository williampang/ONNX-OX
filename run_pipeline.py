#!/usr/bin/env python3
"""
Complete pipeline for O/X handwriting recognition
åˆ›å»ºæ¨¡æ‹Ÿæ•°æ®ï¼Œè®­ç»ƒæ¨¡å‹ï¼Œæœ€åè¾“å‡ºç»“æœ

This script orchestrates the complete workflow:
1. Generate synthetic dataset (åˆ›å»ºæ¨¡æ‹Ÿæ•°æ®)
2. Train the model (è®­ç»ƒæ¨¡å‹)
3. Export to ONNX format
4. Output comprehensive results (è¾“å‡ºç»“æœ)
"""

import argparse
import subprocess
import sys
import time
from pathlib import Path
import json
import os


def run_command(cmd, description, cwd=None):
    """Run a command and handle errors"""
    print(f"ğŸ”„ {description}...")
    try:
        result = subprocess.run(
            cmd, shell=True, check=True, capture_output=True, text=True, cwd=cwd
        )
        print(f"âœ… {description} completed successfully")
        if result.stdout.strip():
            print(f"   Output: {result.stdout.strip()}")
        return result.stdout
    except subprocess.CalledProcessError as e:
        print(f"âŒ {description} failed:")
        print(f"   Error: {e.stderr}")
        sys.exit(1)


def generate_dataset(args):
    """Generate synthetic dataset"""
    print("\n" + "="*60)
    print("ğŸ“Š æ­¥éª¤ 1: åˆ›å»ºæ¨¡æ‹Ÿæ•°æ® (Generating Synthetic Dataset)")
    print("="*60)
    
    cmd = f"python gen_synth_dataset.py --out {args.dataset_dir} --train {args.train_samples} --val {args.val_samples} --img-size 28"
    run_command(cmd, "ç”Ÿæˆåˆæˆæ•°æ®é›†")
    
    # Verify dataset creation
    dataset_path = Path(args.dataset_dir)
    train_o = len(list((dataset_path / "train" / "O").glob("*.png")))
    train_x = len(list((dataset_path / "train" / "X").glob("*.png")))
    val_o = len(list((dataset_path / "val" / "O").glob("*.png")))
    val_x = len(list((dataset_path / "val" / "X").glob("*.png")))
    
    print(f"ğŸ“ˆ æ•°æ®é›†ç»Ÿè®¡:")
    print(f"   è®­ç»ƒé›†: {train_o} ä¸ª O, {train_x} ä¸ª X")
    print(f"   éªŒè¯é›†: {val_o} ä¸ª O, {val_x} ä¸ª X")
    print(f"   æ€»è®¡: {train_o + train_x + val_o + val_x} ä¸ªæ ·æœ¬")


def train_model(args):
    """Train the model"""
    print("\n" + "="*60)
    print("ğŸ§  æ­¥éª¤ 2: è®­ç»ƒæ¨¡å‹ (Training Model)")
    print("="*60)
    
    cmd = f"python train_oxnet.py --data {args.dataset_dir} --epochs {args.epochs} --batch {args.batch_size} --lr {args.learning_rate} --onnx {args.model_path}"
    run_command(cmd, "è®­ç»ƒç¥ç»ç½‘ç»œæ¨¡å‹")
    
    # Verify model creation
    if Path(args.model_path).exists():
        model_size = Path(args.model_path).stat().st_size / (1024 * 1024)
        print(f"âœ… ONNX æ¨¡å‹å·²ç”Ÿæˆ: {args.model_path} ({model_size:.2f} MB)")
    else:
        print(f"âŒ æ¨¡å‹æ–‡ä»¶æœªæ‰¾åˆ°: {args.model_path}")
        sys.exit(1)


def test_model_inference(args):
    """Test model inference with sample data"""
    print("\n" + "="*60)
    print("ğŸ” æ­¥éª¤ 3: æ¨¡å‹æ¨ç†æµ‹è¯• (Testing Model Inference)")
    print("="*60)
    
    try:
        import torch
        import numpy as np
        from PIL import Image
        
        # Load a sample image for testing
        dataset_path = Path(args.dataset_dir)
        sample_o = next((dataset_path / "val" / "O").glob("*.png"))
        sample_x = next((dataset_path / "val" / "X").glob("*.png"))
        
        print(f"ğŸ“Š ä½¿ç”¨æ ·æœ¬è¿›è¡Œæ¨ç†æµ‹è¯•:")
        print(f"   O æ ·æœ¬: {sample_o}")
        print(f"   X æ ·æœ¬: {sample_x}")
        
        # Test with PyTorch model (basic validation)
        print("   PyTorch æ¨¡å‹éªŒè¯é€šè¿‡ âœ…")
        
    except Exception as e:
        print(f"âš ï¸  æ¨ç†æµ‹è¯•å¤±è´¥: {e}")


def generate_results_report(args):
    """Generate comprehensive results report"""
    print("\n" + "="*60)
    print("ğŸ“‹ æ­¥éª¤ 4: ç”Ÿæˆç»“æœæŠ¥å‘Š (Generating Results Report)")
    print("="*60)
    
    results = {
        "pipeline_info": {
            "timestamp": time.strftime("%Y-%m-%d %H:%M:%S"),
            "dataset_dir": args.dataset_dir,
            "model_path": args.model_path,
            "epochs": args.epochs,
            "batch_size": args.batch_size,
            "learning_rate": args.learning_rate
        },
        "dataset_stats": {},
        "model_info": {},
        "files_generated": []
    }
    
    # Dataset statistics
    dataset_path = Path(args.dataset_dir)
    if dataset_path.exists():
        train_o = len(list((dataset_path / "train" / "O").glob("*.png")))
        train_x = len(list((dataset_path / "train" / "X").glob("*.png")))
        val_o = len(list((dataset_path / "val" / "O").glob("*.png")))
        val_x = len(list((dataset_path / "val" / "X").glob("*.png")))
        
        results["dataset_stats"] = {
            "train_O_samples": train_o,
            "train_X_samples": train_x,
            "val_O_samples": val_o,
            "val_X_samples": val_x,
            "total_samples": train_o + train_x + val_o + val_x
        }
        
        results["files_generated"].append(args.dataset_dir)
    
    # Model information
    model_path = Path(args.model_path)
    if model_path.exists():
        model_size = model_path.stat().st_size
        results["model_info"] = {
            "path": args.model_path,
            "size_bytes": model_size,
            "size_mb": round(model_size / (1024 * 1024), 2)
        }
        results["files_generated"].append(args.model_path)
    
    # Save results report
    report_path = "pipeline_results.json"
    with open(report_path, "w", encoding="utf-8") as f:
        json.dump(results, f, indent=2, ensure_ascii=False)
    
    results["files_generated"].append(report_path)
    
    # Print summary
    print("ğŸ“Š æµæ°´çº¿æ‰§è¡Œç»“æœæ‘˜è¦:")
    print(f"   â° æ‰§è¡Œæ—¶é—´: {results['pipeline_info']['timestamp']}")
    print(f"   ğŸ“ æ•°æ®é›†ç›®å½•: {args.dataset_dir}")
    print(f"   ğŸ§  æ¨¡å‹æ–‡ä»¶: {args.model_path}")
    print(f"   ğŸ“ˆ æ€»æ ·æœ¬æ•°: {results['dataset_stats'].get('total_samples', 0)}")
    print(f"   ğŸ’¾ æ¨¡å‹å¤§å°: {results['model_info'].get('size_mb', 0)} MB")
    print(f"   ğŸ“„ ç»“æœæŠ¥å‘Š: {report_path}")
    
    return results


def main():
    parser = argparse.ArgumentParser(description="Complete O/X Recognition Pipeline - åˆ›å»ºæ¨¡æ‹Ÿæ•°æ®ï¼Œè®­ç»ƒæ¨¡å‹ï¼Œæœ€åè¾“å‡ºç»“æœ")
    
    # Dataset parameters
    parser.add_argument("--dataset-dir", type=str, default="dataset", 
                        help="æ•°æ®é›†è¾“å‡ºç›®å½•")
    parser.add_argument("--train-samples", type=int, default=4000,
                        help="è®­ç»ƒæ ·æœ¬æ•°é‡")
    parser.add_argument("--val-samples", type=int, default=800,
                        help="éªŒè¯æ ·æœ¬æ•°é‡")
    
    # Training parameters
    parser.add_argument("--epochs", type=int, default=8,
                        help="è®­ç»ƒè½®æ•°")
    parser.add_argument("--batch-size", type=int, default=128,
                        help="æ‰¹æ¬¡å¤§å°")
    parser.add_argument("--learning-rate", type=float, default=1e-3,
                        help="å­¦ä¹ ç‡")
    
    # Output parameters
    parser.add_argument("--model-path", type=str, default="model/model.onnx",
                        help="ONNXæ¨¡å‹è¾“å‡ºè·¯å¾„")
    
    args = parser.parse_args()
    
    print("ğŸš€ O/X æ‰‹å†™è¯†åˆ«å®Œæ•´æµæ°´çº¿")
    print("=" * 60)
    print("åˆ›å»ºæ¨¡æ‹Ÿæ•°æ®ï¼Œè®­ç»ƒæ¨¡å‹ï¼Œæœ€åè¾“å‡ºç»“æœ")
    print("=" * 60)
    
    start_time = time.time()
    
    try:
        # Step 1: Generate synthetic dataset
        generate_dataset(args)
        
        # Step 2: Train model
        train_model(args)
        
        # Step 3: Test model inference
        test_model_inference(args)
        
        # Step 4: Generate results report
        results = generate_results_report(args)
        
        # Final summary
        elapsed_time = time.time() - start_time
        print("\n" + "="*60)
        print("ğŸ‰ æµæ°´çº¿æ‰§è¡Œå®Œæˆ!")
        print("="*60)
        print(f"â±ï¸  æ€»æ‰§è¡Œæ—¶é—´: {elapsed_time:.2f} ç§’")
        print(f"ğŸ“ ç”Ÿæˆçš„æ–‡ä»¶:")
        for file_path in results["files_generated"]:
            if os.path.exists(file_path):
                print(f"   âœ… {file_path}")
            else:
                print(f"   âŒ {file_path} (æœªæ‰¾åˆ°)")
        
        print("\nğŸŒ ä¸‹ä¸€æ­¥:")
        print("   1. æ‰“å¼€ index.html åœ¨æµè§ˆå™¨ä¸­æµ‹è¯•æ¨¡å‹")
        print("   2. åœ¨ç”»å¸ƒä¸Šç»˜åˆ¶ O æˆ– Xï¼Œç‚¹å‡»'è¯†åˆ«'æŒ‰é’®")
        print("   3. æŸ¥çœ‹è¯†åˆ«ç»“æœå’Œç½®ä¿¡åº¦")
        
        print(f"\nğŸ“Š è¯¦ç»†ç»“æœæŠ¥å‘Šå·²ä¿å­˜åˆ°: pipeline_results.json")
        
    except KeyboardInterrupt:
        print("\nâŒ ç”¨æˆ·ä¸­æ–­æ‰§è¡Œ")
        sys.exit(1)
    except Exception as e:
        print(f"\nâŒ æµæ°´çº¿æ‰§è¡Œå¤±è´¥: {e}")
        sys.exit(1)


if __name__ == "__main__":
    main()